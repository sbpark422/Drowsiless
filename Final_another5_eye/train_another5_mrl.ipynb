{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Activation, Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "tb_hist = TensorBoard(log_dir='./graphFINAL', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "# Load Dataset\n",
    "\n",
    "import glob\n",
    "\n",
    "imgs = glob.glob('./mrlEyes_2018_01/*/*.png')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_imgs, test_imgs = train_test_split(imgs, test_size=0.15)\n",
    "\n",
    "len(train_imgs), len(test_imgs)\n",
    "\n",
    "train_imgs, valid_imgs = train_test_split(train_imgs, test_size=0.18)\n",
    "\n",
    "len(train_imgs), len(valid_imgs), len(test_imgs)\n",
    "\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "x_train = np.empty((len(train_imgs), 32, 32, 1))\n",
    "y_train = np.empty(len(train_imgs))\n",
    "\n",
    "x_valid = np.empty((len(valid_imgs), 32, 32, 1))\n",
    "y_valid = np.empty(len(valid_imgs))\n",
    "\n",
    "x_test = np.empty((len(test_imgs), 32, 32, 1))\n",
    "y_test = np.empty(len(test_imgs))\n",
    "\n",
    "for idx, train in enumerate(train_imgs):\n",
    "    x_train[idx] = np.expand_dims(np.array(Image.open(train).resize((32, 32), Image.BICUBIC)), -1)\n",
    "    y_train[idx] = int(train.split('/')[-1].split('_')[4])\n",
    "\n",
    "for idx, valid in enumerate(valid_imgs):\n",
    "    x_valid[idx] = np.expand_dims(np.array(Image.open(valid).resize((32, 32), Image.BICUBIC)), -1)\n",
    "    y_valid[idx] = int(valid.split('/')[-1].split('_')[4])\n",
    "\n",
    "for idx, test in enumerate(test_imgs):\n",
    "    x_test[idx] = np.expand_dims(np.array(Image.open(test).resize((32, 32), Image.BICUBIC)), -1)\n",
    "    y_test[idx] = int(test.split('/')[-1].split('_')[4])\n",
    "\n",
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape, x_test.shape, y_test.shape\n",
    "\n",
    "# Preview\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(str(int(y_train[0])))\n",
    "plt.imshow(x_train[0].reshape((32, 32)), cmap='gray')\n",
    "print(\"\\n\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(str(int(y_valid[4])))\n",
    "plt.imshow(x_valid[4].reshape((32, 32)), cmap='gray')\n",
    "\n",
    "# Data Augmentation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2\n",
    ")\n",
    "\n",
    "\n",
    "#valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(\n",
    "    x=x_train, y=y_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow(\n",
    "    x=x_valid, y=y_valid,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow(\n",
    "    x=x_test, y=y_test,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Build Model\n",
    "\n",
    "inputs = Input(shape=(32, 32, 1))\n",
    "\n",
    "# net = Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n",
    "# net = MaxPooling2D(pool_size=2)(net)\n",
    "\n",
    "# net = Conv2D(64, kernel_size=3, strides=1, padding='same', activation='relu')(net)\n",
    "# net = MaxPooling2D(pool_size=2)(net)\n",
    "\n",
    "# net = Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')(net)\n",
    "# net = MaxPooling2D(pool_size=2)(net)\n",
    "\n",
    "# net = Flatten()(net)\n",
    "\n",
    "# net = Dense(512)(net)\n",
    "# net = Activation('relu')(net)\n",
    "# net = Dense(1)(net)\n",
    "# outputs = Activation('sigmoid')(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,142,273\n",
      "Trainable params: 1,142,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net = Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')(inputs)\n",
    "net = MaxPooling2D(pool_size=2)(net)\n",
    "#net = BatchNormalization(axis=chanDim)(net)\n",
    "net = Dropout(0.25)(net)\n",
    "\n",
    "net = Conv2D(64, kernel_size=3, strides=1, padding='same', activation='relu')(net)\n",
    "net = MaxPooling2D(pool_size=2)(net)\n",
    "#net = BatchNormalization(axis=chanDim)(net)\n",
    "net= Dropout(0.25)(net)\n",
    "\n",
    "\n",
    "net = Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')(net)\n",
    "net = MaxPooling2D(pool_size=2)(net)\n",
    "#net = BatchNormalization(axis=chanDim)(net)\n",
    "net = Dropout(0.25)(net)\n",
    "\n",
    "net = Flatten()(net)\n",
    "\n",
    "net = Dense(512)(net)\n",
    "net = Activation('relu')(net)\n",
    "#net = BatchNormalization()(net)\n",
    "net = Dropout(0.5)(net)\n",
    "\n",
    "\n",
    "#classifier\n",
    "net = Dense(1)(net)\n",
    "outputs = Activation('sigmoid')(net)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "plot_model(model, to_file = './model_customize.png', show_shapes =True, show_layer_names=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1850/1850 [==============================] - 28s 15ms/step - loss: 0.4246 - acc: 0.7952 - val_loss: 0.2325 - val_acc: 0.9142\n",
      "Epoch 2/50\n",
      "  19/1850 [..............................] - ETA: 16s - loss: 0.2715 - acc: 0.8931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:432: RuntimeWarning: Can save best model only with valid_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:1043: RuntimeWarning: Reduce LR on plateau conditioned on metric `valid_acc` which is not available. Available metrics are: lr,loss,acc,val_acc,val_loss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.2145 - acc: 0.9149 - val_loss: 0.1336 - val_acc: 0.9506\n",
      "Epoch 3/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.1616 - acc: 0.9375 - val_loss: 0.0986 - val_acc: 0.9647\n",
      "Epoch 4/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.1324 - acc: 0.9495 - val_loss: 0.0915 - val_acc: 0.9679\n",
      "Epoch 5/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.1221 - acc: 0.9547 - val_loss: 0.0955 - val_acc: 0.9638\n",
      "Epoch 6/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.1156 - acc: 0.9572 - val_loss: 0.0695 - val_acc: 0.9740\n",
      "Epoch 7/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.1094 - acc: 0.9596 - val_loss: 0.0735 - val_acc: 0.9723\n",
      "Epoch 8/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.1001 - acc: 0.9629 - val_loss: 0.0605 - val_acc: 0.9782\n",
      "Epoch 9/50\n",
      "1850/1850 [==============================] - 27s 14ms/step - loss: 0.0972 - acc: 0.9636 - val_loss: 0.0625 - val_acc: 0.9770\n",
      "Epoch 10/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0960 - acc: 0.9655 - val_loss: 0.0586 - val_acc: 0.9774\n",
      "Epoch 11/50\n",
      "1850/1850 [==============================] - 27s 14ms/step - loss: 0.0936 - acc: 0.9645 - val_loss: 0.0594 - val_acc: 0.9787\n",
      "Epoch 12/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0871 - acc: 0.9679 - val_loss: 0.0524 - val_acc: 0.9821\n",
      "Epoch 13/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0884 - acc: 0.9677 - val_loss: 0.0500 - val_acc: 0.9823\n",
      "Epoch 14/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0841 - acc: 0.9692 - val_loss: 0.0517 - val_acc: 0.9825\n",
      "Epoch 15/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0822 - acc: 0.9700 - val_loss: 0.0618 - val_acc: 0.9768\n",
      "Epoch 16/50\n",
      "1850/1850 [==============================] - 27s 14ms/step - loss: 0.0808 - acc: 0.9705 - val_loss: 0.0487 - val_acc: 0.9831\n",
      "Epoch 17/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0813 - acc: 0.9708 - val_loss: 0.0528 - val_acc: 0.9801\n",
      "Epoch 18/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0824 - acc: 0.9703 - val_loss: 0.0535 - val_acc: 0.9818\n",
      "Epoch 19/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0796 - acc: 0.9705 - val_loss: 0.0569 - val_acc: 0.9805\n",
      "Epoch 20/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0767 - acc: 0.9724 - val_loss: 0.0469 - val_acc: 0.9828\n",
      "Epoch 21/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0779 - acc: 0.9716 - val_loss: 0.0488 - val_acc: 0.9826\n",
      "Epoch 22/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0762 - acc: 0.9715 - val_loss: 0.0501 - val_acc: 0.9828\n",
      "Epoch 23/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0744 - acc: 0.9735 - val_loss: 0.0514 - val_acc: 0.9828\n",
      "Epoch 24/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0749 - acc: 0.9725 - val_loss: 0.0523 - val_acc: 0.9814\n",
      "Epoch 25/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0718 - acc: 0.9741 - val_loss: 0.0478 - val_acc: 0.9821\n",
      "Epoch 26/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0736 - acc: 0.9734 - val_loss: 0.0502 - val_acc: 0.9820\n",
      "Epoch 27/50\n",
      "1850/1850 [==============================] - 26s 14ms/step - loss: 0.0735 - acc: 0.9730 - val_loss: 0.0467 - val_acc: 0.9828\n",
      "Epoch 28/50\n",
      " 109/1850 [>.............................] - ETA: 20s - loss: 0.0766 - acc: 0.9702"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train\n",
    "\n",
    "start_time = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "epochs_num = 50\n",
    "hist = model.fit_generator(\n",
    "    train_generator, epochs= epochs_num , validation_data=valid_generator,\n",
    "    callbacks=[\n",
    "        ModelCheckpoint('models/%s.h5' % (start_time), monitor='val_loss', save_best_only=True, mode='max', verbose=1),\n",
    "        ReduceLROnPlateau(monitor='valid_acc', factor=0.2, patience=10, verbose=1, mode='auto', min_lr=1e-05), tb_hist\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.save('models/train_mrl_%s.h5' %(epochs_num))\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title(str(int(y_train[0])))\n",
    "plt.imshow(x_train[0].reshape((32, 32)), cmap='gray')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title(str(int(y_valid[4])))\n",
    "plt.imshow(x_valid[4].reshape((32, 32)), cmap='gray')\n",
    "\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.8, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "scores = model.evaluate_generator(test_generator, steps=10)\n",
    "print(\"## evaluation ##\")\n",
    "print(scores)\n",
    "print(\"##  accuracy  ##\")\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "scores = model.evaluate_generator(test_generator)\n",
    "print(\"## evaluation ##\")\n",
    "print(scores)\n",
    "print(\"##  accuracy  ##\")\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
    "#\n",
    "#model.fit_generator(\n",
    "#    train_generator, epochs=50, validation_data=valid_generator,\n",
    "#    callbacks=[\n",
    "#        ModelCheckpoint('models/%s.h5' % (start_time), monitor='valid_acc', save_best_only=True, mode='max', verbose=1),\n",
    "#        ReduceLROnPlateau(monitor='valid_acc', factor=0.2, patience=10, verbose=1, mode='auto', min_lr=1e-05)\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('models/train_mrl_0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(‘train_mrl_0.hdf5’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history['loss'], history['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "#import seaborn as sns\n",
    "\n",
    "#model = load_model('models/%s.h5' % (start_time))\n",
    "\n",
    "#y_pred = model.predict(x_val/255.)\n",
    "#y_pred_logical = (y_pred > 0.5).astype(np.int)\n",
    "\n",
    "#print ('test acc: %s' % accuracy_score(y_val, y_pred_logical))\n",
    "#cm = confusion_matrix(y_val, y_pred_logical)\n",
    "#sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = sns.distplot(y_pred, kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.applications import MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MobileNetV2(input_shape=(32, 32, 1), include_top=True, weights=None, classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
